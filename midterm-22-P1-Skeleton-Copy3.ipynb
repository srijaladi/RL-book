{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import V, S, A\n",
    "from rl import dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import markov_process, markov_decision_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.iterate import converged, iterate\n",
    "from rl.markov_process import NonTerminal, State\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess,\n",
    "                                        FiniteMarkovRewardProcess)\n",
    "from rl.policy import FinitePolicy, FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.midterm_2022.priority_q import  PriorityQueue\n",
    "from rl.midterm_2022 import grid_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will implement the gaps-based value iteration algorithm mentioned in class.\n",
    "\n",
    "The gaps-based iteration algorithm proceeds as follows\n",
    "\n",
    "1. Initialize the value function to zero for all states: $v[s] = 0\\ \\forall s \\in \\mathcal{N}$\n",
    "2. Calculate the gaps for each state: $g[s] = |v[s] - \\max_a \\mathcal{R}(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') \\cdot v(s')|$\n",
    "3. While there is some gap that exceeds a threshold\n",
    " - Select the state with the  largest gap: $s_{max} = \\arg\\max_{s \\in \\mathcal{N}} g[s]$\n",
    " - Update the value function for $s_{max}$: $v[s_{max}] = \\max_a \\mathcal{R}(s_{max},a) + \\sum_{s'}\\mathcal{P}(s_{max},a,s') \\cdot v(s')$\n",
    " -  Update the gap for $s_{max}$: $g[s_{max}] = 0$\n",
    "4. Return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your implementation on a grid maze MDP. We have defined this class in \"grid_maze.py\", you should  briefly familiarize yourself with that code. In particular pay attention to the difference in reward functions for the two classes \"GridMazeMDP_Dense\" and \"GridMazeMDP_Sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(10, 10)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can visualize the maze if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|*| |         |     |\n",
      "| + + +-+-+ +-+-+ +-+\n",
      "|     | | | | |   | |\n",
      "| + + + + +-+ +-+ + +\n",
      "| | |               |\n",
      "| +-+ +-+ + + +-+ +-+\n",
      "| | | |   | | | |   |\n",
      "| + +-+ +-+ + + +-+ +\n",
      "|     |   | |   |   |\n",
      "| +-+ + +-+-+ +-+-+ +\n",
      "|   | | |       |   |\n",
      "|-+ + + +-+ +-+ + +-+\n",
      "|   | |   | |   |   |\n",
      "|-+ +-+ +-+ +-+ +-+-+\n",
      "|     |   | |       |\n",
      "| +-+ + +-+ +-+ +-+-+\n",
      "| | | | | | |     | |\n",
      "| + + + + +-+-+ + + +\n",
      "| |   | |       |   |\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also visualize a policy on the mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|*|v|v < < < <|> v <|\n",
      "| + + +-+-+ +-+-+ +-+\n",
      "|^ < <|v|v|^|v|> v|v|\n",
      "| + + + + +-+ +-+ + +\n",
      "|^|^|^ < < < < < < <|\n",
      "| +-+ +-+ + + +-+ +-+\n",
      "|^|v|^|> ^|^|^|v|^ <|\n",
      "| + +-+ +-+ + + +-+ +\n",
      "|^ < <|^ <|^|^ <|> ^|\n",
      "| +-+ + +-+-+ +-+-+ +\n",
      "|^ <|^|^|> > ^ <|> ^|\n",
      "|-+ + + +-+ +-+ + +-+\n",
      "|> ^|^|^ <|^|> ^|^ <|\n",
      "|-+ +-+ +-+ +-+ +-+-+\n",
      "|> ^ <|^ <|^|> ^ < <|\n",
      "| +-+ + +-+ +-+ +-+-+\n",
      "|^|v|^|^|v|^|> ^ <|v|\n",
      "| + + + + +-+-+ + + +\n",
      "|^|> ^|^|> > > ^|^ <|\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(maze_mdp.print_policy(v2_res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations\n",
    "1. isEmpty(self): check if the queue is empty   \n",
    "2. contains(self, element): check if the queue contains an element\n",
    "3. peek(self): peek at the highest priority element in the queue    \n",
    "4. pop(self): remove and return the highest priority element in the queue    \n",
    "5. insert(self, element, priority): insert an element into the queue with given priority\n",
    "6. update(self, element, new_priority): update the priority of an element in the queue\n",
    "7. delete(self, element): delete an element from the queue\n",
    "\n",
    "Below are some examples of using the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : the queue is empty\n",
      "False : the queue is not empty\n",
      "True : the queue contains a\n",
      "False : the queue does not contain a\n",
      "(1, 'a') : a is the first element in the queue\n",
      "True : the queue now contains b\n",
      "(0, 'b') : b is now at the front of the queue\n",
      "b : we removed b from the queue\n",
      "False : the queue still nonempty\n",
      "True : the queue still contains a\n",
      "False : the queue does not contain b anymore\n",
      "(1, 'a') : a is at the front of the queue\n",
      "(1, 'a') : a is still at the front of the queue\n",
      "(5, 'c') : after updating a is no longer at the front of the queue\n"
     ]
    }
   ],
   "source": [
    "q: PriorityQueue = PriorityQueue()\n",
    "print(q.isEmpty(), ':', \"the queue is empty\")\n",
    "q.insert(\"a\", 1)\n",
    "print(q.isEmpty(), ':',  \"the queue is not empty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain a\")\n",
    "print(q.peek(), ':',  \"a is the first element in the queue\")\n",
    "q.insert(\"b\", 0)\n",
    "print(q.contains(\"b\"), ':',  \"the queue now contains b\")\n",
    "print(q.peek(), ':',  \"b is now at the front of the queue\")\n",
    "x = q.pop()\n",
    "print(x, ':',  \"we removed b from the queue\")\n",
    "print(q.isEmpty(), ':',  \"the queue still nonempty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue still contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain b anymore\")\n",
    "print(q.peek(), ':',  \"a is at the front of the queue\")\n",
    "q.insert(\"c\", 5)\n",
    "print(q.peek(), ':',  \"a is still at the front of the queue\")\n",
    "q.update(\"a\", 6)\n",
    "print(q.peek(), ':',  \"after updating a is no longer at the front of the queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def invert_transition_map(mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A]) ->\\\n",
    "            Mapping[S, Iterable[S]]:\n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    Implement the invert_transition_map method\n",
    "    '''\n",
    "    inverted_mapping: Dict[S, Iterable[S]] = {}\n",
    "    mdp_mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]] = mdp.mapping\n",
    "    \n",
    "    for currState in mdp_mapping.keys():\n",
    "        inverted_mapping[currState.state] = set()\n",
    "    \n",
    "    for currState in mdp_mapping.keys():\n",
    "        action_map = mdp_mapping[currState]\n",
    "        \n",
    "        possibleNewStates: Set[S] = set()\n",
    "            \n",
    "        for dist_map in action_map.values():\n",
    "            for new_state_and_reward, prob in dist_map:\n",
    "                new_state = new_state_and_reward[0]\n",
    "                if prob > 0 and (new_state in mdp_mapping.keys()):\n",
    "                    inverted_mapping[new_state.state].add(currState.state)       \n",
    "        \n",
    "    #raise NotImplementedError\n",
    "    '''END YOUR CODE'''\n",
    "    return inverted_mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaps_value_iteration(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float, \n",
    "    gaps: PriorityQueue) -> Iterator[V[S]]:\n",
    "    '''\n",
    "    Calculate the value function (V*) of the given MDP by applying the\n",
    "    update function repeatedly until the values converge.\n",
    "\n",
    "    '''\n",
    "    dependency_map = invert_transition_map(mdp)\n",
    "    #print(dependency_map)\n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    #print(v_0)\n",
    "        \n",
    "    def update(v: V[S]) -> V[S]:\n",
    "        #print(v)\n",
    "        #print(mdp.mapping)\n",
    "        maxStateGap = gaps.pop()\n",
    "        #print(maxStateGap)\n",
    "        #print(gaps)\n",
    "        maxState: S = maxStateGap\n",
    "            \n",
    "        maxState_action_map = mdp.mapping[NonTerminal(maxState)]\n",
    "        \n",
    "        def bestActionValue(state_action_map): \n",
    "            action_value_map: Dict[A, float] = {}\n",
    "            max_val = float('-inf')\n",
    "            for dist_map in state_action_map.values():\n",
    "                #dist_map = state_action_map[action]\n",
    "\n",
    "                reward: float = 0\n",
    "                future_value: float = 0\n",
    "\n",
    "                for stateReward, prob in dist_map:\n",
    "                    reward += prob * stateReward[1]\n",
    "                    if stateReward[0] in mdp.non_terminal_states:\n",
    "                        future_value += gamma * prob * v[stateReward[0]]\n",
    "\n",
    "                max_val = max(future_value + reward, max_val)\n",
    "            \n",
    "            return max_val\n",
    "            #bestAction = max(action_value_map, key=action_value_map.get)\n",
    "            #bestValue = action_value_map[bestAction]\n",
    "            \n",
    "            #return max(action_value_map.values())\n",
    "            #return bestValue\n",
    "        \n",
    "        \n",
    "        #print(\"Old value for: \" + str(maxState) + \" is: \" + str(v[NonTerminal(maxState)]))\n",
    "        #print(\"Gap is: \" + str(maxStateGap[0]))\n",
    "        v[NonTerminal(maxState)] = bestActionValue(maxState_action_map)\n",
    "        #print(\"New value for: \" + str(maxState) + \" is: \" + str(v[NonTerminal(maxState)]))\n",
    "        #gaps.update(maxState, 0)\n",
    "        \n",
    "        #print(\"MaxState: \" + str(maxState))\n",
    "        #print(dependency_map[maxState])\n",
    "        \n",
    "        for dependent_state in dependency_map[maxState]:\n",
    "            dependent_state_best_val = bestActionValue(mdp.mapping[NonTerminal(dependent_state)])\n",
    "            stateNewGap = abs(v[NonTerminal(dependent_state)] - dependent_state_best_val)\n",
    "            if stateNewGap == 0:\n",
    "                continue\n",
    "            if not(gaps.isEmpty()) and gaps.contains(dependent_state):\n",
    "                gaps.update(dependent_state, -stateNewGap)\n",
    "            else:\n",
    "                gaps.insert(dependent_state, -stateNewGap)\n",
    "\n",
    "            \n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        perform a single update to v for the state with the largest gap\n",
    "        update the gaps for any dependent states\n",
    "        '''\n",
    "        #raise NotImplementedError \n",
    "        '''END YOUR CODE'''\n",
    "        return v\n",
    "\n",
    "    \n",
    "    return iterate(update, v_0)\n",
    "\n",
    "\n",
    "def gaps_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    \n",
    "    gaps = PriorityQueue()\n",
    "\n",
    "    v: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    \n",
    "    def bestActionValue(state_action_map): \n",
    "        action_value_map: Dict[A, float] = {}\n",
    "\n",
    "        for action in state_action_map.keys():\n",
    "            dist_map = state_action_map[action]\n",
    "\n",
    "            reward: float = 0\n",
    "            future_value: float = 0\n",
    "\n",
    "            for stateReward, prob in dist_map:\n",
    "                reward += prob * stateReward[1]\n",
    "                if stateReward[0] in mdp.non_terminal_states:\n",
    "                    future_value += gamma * prob * v[stateReward[0]]\n",
    "\n",
    "                #future_value += gamma * prob * v[stateReward[0].state]\n",
    "\n",
    "            action_value_map[action] = future_value + reward\n",
    "\n",
    "        bestAction = max(action_value_map, key=action_value_map.get)\n",
    "        bestValue = action_value_map[bestAction]\n",
    "\n",
    "        return max(action_value_map.values())\n",
    "        return bestValue\n",
    "    \n",
    "    for s in mdp.non_terminal_states:\n",
    "        sBestVal = -abs(bestActionValue(mdp.mapping[s]))\n",
    "        if not(gaps.isEmpty()) and sBestVal < gaps.peek()[0]:\n",
    "            gaps.pop()\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "        if not(gaps.isEmpty()) and sBestVal == gaps.peek()[0] and not(gaps.contains(s.state)):\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "        elif gaps.isEmpty():\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "        \n",
    "    #print(gaps)\n",
    "    \n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    instantiate the value function and populate the gaps\n",
    "    ''' \n",
    "    #raise NotImplementedError\n",
    "    ''' END YOUR CODE ''' \n",
    "    \n",
    "    def criterion(x,y):\n",
    "        THRESHOLD = 1e-5\n",
    "        return (gaps.isEmpty()) or (abs(gaps.peek()[0]) < THRESHOLD)\n",
    "        ''' END YOUR CODE ''' \n",
    "        \n",
    "    opt_vf: V[S] = converged(\n",
    "        gaps_value_iteration(mdp, gamma, gaps),\n",
    "        done= criterion \n",
    "    )\n",
    "        \n",
    "    opt_policy: markov_decision_process.FiniteDeterministicPolicy[S, A] = dynamic_programming.greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change the code below here, just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(50, 50)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)\n",
    "#print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.029789209365845\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(v2_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.931114912033081\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "#print(v2_res)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_mdp = grid_maze.GridMazeMDP_Dense(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657.9183399677277\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.062140941619873\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=GridState(x=0, y=1)): -1.0, NonTerminal(state=GridState(x=0, y=2)): -2.0, NonTerminal(state=GridState(x=0, y=3)): -3.0, NonTerminal(state=GridState(x=0, y=4)): -6.0, NonTerminal(state=GridState(x=0, y=5)): -7.0, NonTerminal(state=GridState(x=0, y=6)): -8.0, NonTerminal(state=GridState(x=0, y=7)): -9.0, NonTerminal(state=GridState(x=0, y=8)): -12.0, NonTerminal(state=GridState(x=0, y=9)): -13.0, NonTerminal(state=GridState(x=0, y=10)): -14.0, NonTerminal(state=GridState(x=0, y=11)): -17.0, NonTerminal(state=GridState(x=1, y=0)): -1.0, NonTerminal(state=GridState(x=1, y=1)): -2.0, NonTerminal(state=GridState(x=1, y=2)): -3.0, NonTerminal(state=GridState(x=1, y=3)): -4.0, NonTerminal(state=GridState(x=1, y=4)): -5.0, NonTerminal(state=GridState(x=1, y=5)): -6.0, NonTerminal(state=GridState(x=1, y=6)): -9.0, NonTerminal(state=GridState(x=1, y=7)): -10.0, NonTerminal(state=GridState(x=1, y=8)): -11.0, NonTerminal(state=GridState(x=1, y=9)): -12.0, NonTerminal(state=GridState(x=1, y=10)): -15.0, NonTerminal(state=GridState(x=1, y=11)): -16.0, NonTerminal(state=GridState(x=2, y=0)): -2.0, NonTerminal(state=GridState(x=2, y=1)): -5.0, NonTerminal(state=GridState(x=2, y=2)): -4.0, NonTerminal(state=GridState(x=2, y=3)): -5.0, NonTerminal(state=GridState(x=2, y=4)): -8.0, NonTerminal(state=GridState(x=2, y=5)): -7.0, NonTerminal(state=GridState(x=2, y=6)): -8.0, NonTerminal(state=GridState(x=2, y=7)): -11.0, NonTerminal(state=GridState(x=2, y=8)): -12.0, NonTerminal(state=GridState(x=2, y=9)): -13.0, NonTerminal(state=GridState(x=2, y=10)): -16.0, NonTerminal(state=GridState(x=2, y=11)): -17.0, NonTerminal(state=GridState(x=3, y=0)): -3.0, NonTerminal(state=GridState(x=3, y=1)): -6.0, NonTerminal(state=GridState(x=3, y=2)): -5.0, NonTerminal(state=GridState(x=3, y=3)): -6.0, NonTerminal(state=GridState(x=3, y=4)): -9.0, NonTerminal(state=GridState(x=3, y=5)): -8.0, NonTerminal(state=GridState(x=3, y=6)): -9.0, NonTerminal(state=GridState(x=3, y=7)): -10.0, NonTerminal(state=GridState(x=3, y=8)): -11.0, NonTerminal(state=GridState(x=3, y=9)): -12.0, NonTerminal(state=GridState(x=3, y=10)): -17.0, NonTerminal(state=GridState(x=3, y=11)): -18.0, NonTerminal(state=GridState(x=4, y=0)): -4.0, NonTerminal(state=GridState(x=4, y=1)): -7.0, NonTerminal(state=GridState(x=4, y=2)): -6.0, NonTerminal(state=GridState(x=4, y=3)): -13.0, NonTerminal(state=GridState(x=4, y=4)): -10.0, NonTerminal(state=GridState(x=4, y=5)): -9.0, NonTerminal(state=GridState(x=4, y=6)): -10.0, NonTerminal(state=GridState(x=4, y=7)): -11.0, NonTerminal(state=GridState(x=4, y=8)): -12.0, NonTerminal(state=GridState(x=4, y=9)): -13.0, NonTerminal(state=GridState(x=4, y=10)): -18.0, NonTerminal(state=GridState(x=4, y=11)): -19.0, NonTerminal(state=GridState(x=5, y=0)): -5.0, NonTerminal(state=GridState(x=5, y=1)): -6.0, NonTerminal(state=GridState(x=5, y=2)): -13.0, NonTerminal(state=GridState(x=5, y=3)): -12.0, NonTerminal(state=GridState(x=5, y=4)): -11.0, NonTerminal(state=GridState(x=5, y=5)): -10.0, NonTerminal(state=GridState(x=5, y=6)): -11.0, NonTerminal(state=GridState(x=5, y=7)): -12.0, NonTerminal(state=GridState(x=5, y=8)): -13.0, NonTerminal(state=GridState(x=5, y=9)): -20.0, NonTerminal(state=GridState(x=5, y=10)): -19.0, NonTerminal(state=GridState(x=5, y=11)): -20.0, NonTerminal(state=GridState(x=6, y=0)): -8.0, NonTerminal(state=GridState(x=6, y=1)): -7.0, NonTerminal(state=GridState(x=6, y=2)): -8.0, NonTerminal(state=GridState(x=6, y=3)): -13.0, NonTerminal(state=GridState(x=6, y=4)): -12.0, NonTerminal(state=GridState(x=6, y=5)): -15.0, NonTerminal(state=GridState(x=6, y=6)): -12.0, NonTerminal(state=GridState(x=6, y=7)): -13.0, NonTerminal(state=GridState(x=6, y=8)): -22.0, NonTerminal(state=GridState(x=6, y=9)): -21.0, NonTerminal(state=GridState(x=6, y=10)): -20.0, NonTerminal(state=GridState(x=6, y=11)): -21.0, NonTerminal(state=GridState(x=7, y=0)): -9.0, NonTerminal(state=GridState(x=7, y=1)): -8.0, NonTerminal(state=GridState(x=7, y=2)): -9.0, NonTerminal(state=GridState(x=7, y=3)): -10.0, NonTerminal(state=GridState(x=7, y=4)): -13.0, NonTerminal(state=GridState(x=7, y=5)): -14.0, NonTerminal(state=GridState(x=7, y=6)): -13.0, NonTerminal(state=GridState(x=7, y=7)): -14.0, NonTerminal(state=GridState(x=7, y=8)): -15.0, NonTerminal(state=GridState(x=7, y=9)): -22.0, NonTerminal(state=GridState(x=7, y=10)): -21.0, NonTerminal(state=GridState(x=7, y=11)): -22.0, NonTerminal(state=GridState(x=8, y=0)): -10.0, NonTerminal(state=GridState(x=8, y=1)): -11.0, NonTerminal(state=GridState(x=8, y=2)): -10.0, NonTerminal(state=GridState(x=8, y=3)): -15.0, NonTerminal(state=GridState(x=8, y=4)): -14.0, NonTerminal(state=GridState(x=8, y=5)): -15.0, NonTerminal(state=GridState(x=8, y=6)): -16.0, NonTerminal(state=GridState(x=8, y=7)): -15.0, NonTerminal(state=GridState(x=8, y=8)): -16.0, NonTerminal(state=GridState(x=8, y=9)): -19.0, NonTerminal(state=GridState(x=8, y=10)): -20.0, NonTerminal(state=GridState(x=8, y=11)): -21.0, NonTerminal(state=GridState(x=9, y=0)): -11.0, NonTerminal(state=GridState(x=9, y=1)): -12.0, NonTerminal(state=GridState(x=9, y=2)): -11.0, NonTerminal(state=GridState(x=9, y=3)): -14.0, NonTerminal(state=GridState(x=9, y=4)): -17.0, NonTerminal(state=GridState(x=9, y=5)): -16.0, NonTerminal(state=GridState(x=9, y=6)): -17.0, NonTerminal(state=GridState(x=9, y=7)): -16.0, NonTerminal(state=GridState(x=9, y=8)): -17.0, NonTerminal(state=GridState(x=9, y=9)): -18.0, NonTerminal(state=GridState(x=9, y=10)): -19.0, NonTerminal(state=GridState(x=9, y=11)): -20.0, NonTerminal(state=GridState(x=10, y=0)): -12.0, NonTerminal(state=GridState(x=10, y=1)): -13.0, NonTerminal(state=GridState(x=10, y=2)): -12.0, NonTerminal(state=GridState(x=10, y=3)): -13.0, NonTerminal(state=GridState(x=10, y=4)): -18.0, NonTerminal(state=GridState(x=10, y=5)): -17.0, NonTerminal(state=GridState(x=10, y=6)): -20.0, NonTerminal(state=GridState(x=10, y=7)): -17.0, NonTerminal(state=GridState(x=10, y=8)): -20.0, NonTerminal(state=GridState(x=10, y=9)): -19.0, NonTerminal(state=GridState(x=10, y=10)): -22.0, NonTerminal(state=GridState(x=10, y=11)): -21.0, NonTerminal(state=GridState(x=11, y=0)): -15.0, NonTerminal(state=GridState(x=11, y=1)): -14.0, NonTerminal(state=GridState(x=11, y=2)): -13.0, NonTerminal(state=GridState(x=11, y=3)): -14.0, NonTerminal(state=GridState(x=11, y=4)): -19.0, NonTerminal(state=GridState(x=11, y=5)): -18.0, NonTerminal(state=GridState(x=11, y=6)): -19.0, NonTerminal(state=GridState(x=11, y=7)): -18.0, NonTerminal(state=GridState(x=11, y=8)): -21.0, NonTerminal(state=GridState(x=11, y=9)): -20.0, NonTerminal(state=GridState(x=11, y=10)): -21.0, NonTerminal(state=GridState(x=11, y=11)): -22.0}\n"
     ]
    }
   ],
   "source": [
    "print(v2_res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
