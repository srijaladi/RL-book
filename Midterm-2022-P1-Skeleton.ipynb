{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import V, S, A\n",
    "from rl import dynamic_programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl import markov_process, markov_decision_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Mapping, Iterator, TypeVar, Tuple, Dict, Iterable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.distribution import Categorical, Choose\n",
    "from rl.iterate import converged, iterate\n",
    "from rl.markov_process import NonTerminal, State\n",
    "from rl.markov_decision_process import (FiniteMarkovDecisionProcess,\n",
    "                                        FiniteMarkovRewardProcess)\n",
    "from rl.policy import FinitePolicy, FiniteDeterministicPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.midterm_2022.priority_q import  PriorityQueue\n",
    "from rl.midterm_2022 import grid_maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem we will implement the gaps-based value iteration algorithm mentioned in class.\n",
    "\n",
    "The gaps-based iteration algorithm proceeds as follows\n",
    "\n",
    "1. Initialize the value function to zero for all states: $v[s] = 0\\ \\forall s \\in \\mathcal{N}$\n",
    "2. Calculate the gaps for each state: $g[s] = |v[s] - \\max_a \\mathcal{R}(s,a) + \\sum_{s'} \\mathcal{P}(s,a,s') \\cdot v(s')|$\n",
    "3. While there is some gap that exceeds a threshold\n",
    " - Select the state with the  largest gap: $s_{max} = \\arg\\max_{s \\in \\mathcal{N}} g[s]$\n",
    " - Update the value function for $s_{max}$: $v[s_{max}] = \\max_a \\mathcal{R}(s_{max},a) + \\sum_{s'}\\mathcal{P}(s_{max},a,s') \\cdot v(s')$\n",
    " -  Update the gap for $s_{max}$: $g[s_{max}] = 0$\n",
    "4. Return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your implementation on a grid maze MDP. We have defined this class in \"grid_maze.py\", you should  briefly familiarize yourself with that code. In particular pay attention to the difference in reward functions for the two classes \"GridMazeMDP_Dense\" and \"GridMazeMDP_Sparse\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(10, 10)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can visualize the maze if you wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|*  |     |   | |   |\n",
      "| + +-+ +-+-+ + +-+ +\n",
      "| | | | | |         |\n",
      "| +-+ + + + +-+-+-+-+\n",
      "|           | | | | |\n",
      "| + + + + + + + + + +\n",
      "| | | | | |         |\n",
      "| + +-+ +-+ +-+ + + +\n",
      "| |   |   | |   | | |\n",
      "| + + +-+ +-+-+-+ +-+\n",
      "| | |   |   |       |\n",
      "|-+-+-+-+ +-+-+-+ +-+\n",
      "|           | |   | |\n",
      "|-+-+ + +-+ + +-+ + +\n",
      "|     | | | |       |\n",
      "|-+-+ + + + +-+-+ + +\n",
      "|   | |   |   | | | |\n",
      "| + +-+ +-+ +-+ +-+-+\n",
      "| |     |           |\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also visualize a policy on the mdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "|* <|> v <|> v|v|> v|\n",
      "| + +-+ +-+-+ + +-+ +\n",
      "|^|^|v|v|v|v < < < <|\n",
      "| +-+ + + + +-+-+-+-+\n",
      "|^ < < < < <|v|v|v|v|\n",
      "| + + + + + + + + + +\n",
      "|^|^|^|^|^|^ < < < <|\n",
      "| + +-+ +-+ +-+ + + +\n",
      "|^|^ <|^ <|^|> ^|^|^|\n",
      "| + + +-+ +-+-+-+ +-+\n",
      "|^|^|^ <|^ <|> > ^ <|\n",
      "|-+-+-+-+ +-+-+-+ +-+\n",
      "|> > > > ^ <|v|> ^|v|\n",
      "|-+-+ + +-+ + +-+ + +\n",
      "|> > ^|^|v|^|> > ^ <|\n",
      "|-+-+ + + + +-+-+ + +\n",
      "|> v|^|^ <|^ <|v|^|^|\n",
      "| + +-+ +-+ +-+ +-+-+\n",
      "|^|> > ^|> ^ < < < <|\n",
      "|-+-+-+-+-+-+-+-+-+-+\n"
     ]
    }
   ],
   "source": [
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "print(maze_mdp.print_policy(v2_res[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make use of the PriorityQueue class in your implementation. A PriorityQueue is an ordered queue which supports the following operations\n",
    "1. isEmpty(self): check if the queue is empty   \n",
    "2. contains(self, element): check if the queue contains an element\n",
    "3. peek(self): peek at the highest priority element in the queue    \n",
    "4. pop(self): remove and return the highest priority element in the queue    \n",
    "5. insert(self, element, priority): insert an element into the queue with given priority\n",
    "6. update(self, element, new_priority): update the priority of an element in the queue\n",
    "7. delete(self, element): delete an element from the queue\n",
    "\n",
    "Below are some examples of using the queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True : the queue is empty\n",
      "False : the queue is not empty\n",
      "True : the queue contains a\n",
      "False : the queue does not contain a\n",
      "(1, 'a') : a is the first element in the queue\n",
      "True : the queue now contains b\n",
      "(0, 'b') : b is now at the front of the queue\n",
      "b : we removed b from the queue\n",
      "False : the queue still nonempty\n",
      "True : the queue still contains a\n",
      "False : the queue does not contain b anymore\n",
      "(1, 'a') : a is at the front of the queue\n",
      "(1, 'a') : a is still at the front of the queue\n",
      "(5, 'c') : after updating a is no longer at the front of the queue\n"
     ]
    }
   ],
   "source": [
    "q: PriorityQueue = PriorityQueue()\n",
    "print(q.isEmpty(), ':', \"the queue is empty\")\n",
    "q.insert(\"a\", 1)\n",
    "print(q.isEmpty(), ':',  \"the queue is not empty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain a\")\n",
    "print(q.peek(), ':',  \"a is the first element in the queue\")\n",
    "q.insert(\"b\", 0)\n",
    "print(q.contains(\"b\"), ':',  \"the queue now contains b\")\n",
    "print(q.peek(), ':',  \"b is now at the front of the queue\")\n",
    "x = q.pop()\n",
    "print(x, ':',  \"we removed b from the queue\")\n",
    "print(q.isEmpty(), ':',  \"the queue still nonempty\")\n",
    "print(q.contains(\"a\"), ':',  \"the queue still contains a\")\n",
    "print(q.contains(\"b\"), ':',  \"the queue does not contain b anymore\")\n",
    "print(q.peek(), ':',  \"a is at the front of the queue\")\n",
    "q.insert(\"c\", 5)\n",
    "print(q.peek(), ':',  \"a is still at the front of the queue\")\n",
    "q.update(\"a\", 6)\n",
    "print(q.peek(), ':',  \"after updating a is no longer at the front of the queue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def invert_transition_map(mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A]) ->\\\n",
    "            Mapping[S, Iterable[S]]:\n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    Implement the invert_transition_map method\n",
    "    '''\n",
    "    inverted_mapping: Dict[S, Iterable[S]] = {}\n",
    "    mdp_mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]] = mdp.mapping\n",
    "    \n",
    "    for currState in mdp_mapping.keys():\n",
    "        inverted_mapping[currState.state] = set()\n",
    "    \n",
    "    for currState in mdp_mapping.keys():\n",
    "        action_map = mdp_mapping[currState]\n",
    "        \n",
    "        possibleNewStates: Set[S] = set()\n",
    "            \n",
    "        for dist_map in action_map.values():\n",
    "            for new_state_and_reward, prob in dist_map:\n",
    "                new_state = new_state_and_reward[0]\n",
    "                if prob > 0 and (new_state in mdp_mapping.keys()):\n",
    "                    inverted_mapping[new_state.state].add(currState.state)       \n",
    "        \n",
    "    #raise NotImplementedError\n",
    "    '''END YOUR CODE'''\n",
    "    return inverted_mapping\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.dynamic_programming import V, S, A, extended_vf\n",
    "\n",
    "def gaps_value_iteration(\n",
    "    mdp: markov_decision_process.FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float, \n",
    "    gaps: PriorityQueue) -> Iterator[V[S]]:\n",
    "    '''\n",
    "    Calculate the value function (V*) of the given MDP by applying the\n",
    "    update function repeatedly until the values converge.\n",
    "\n",
    "    '''\n",
    "    dependency_map = invert_transition_map(mdp)\n",
    "    v_0: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "        \n",
    "    def update(v: V[S]) -> V[S]:\n",
    "\n",
    "        maxState: S = gaps.pop()\n",
    "            \n",
    "        maxState_action_map = mdp.mapping[NonTerminal(maxState)]\n",
    "        \n",
    "        def bestActionValue(state_action_map): \n",
    "            action_value_map: Dict[A, float] = {}\n",
    "            max_val = float('-inf')\n",
    "            for dist_map in state_action_map.values():\n",
    "\n",
    "                reward: float = 0\n",
    "                future_value: float = 0\n",
    "\n",
    "                for stateReward, prob in dist_map:\n",
    "                    reward += prob * stateReward[1]\n",
    "                    if isinstance(stateReward[0], NonTerminal):\n",
    "                        future_value += gamma * prob * v[stateReward[0]]\n",
    "\n",
    "                max_val = max(future_value + reward, max_val)\n",
    "            \n",
    "            return max_val\n",
    "        \n",
    "        v[NonTerminal(maxState)] = bestActionValue(maxState_action_map)\n",
    "        \n",
    "        for dependent_state in dependency_map[maxState]:\n",
    "            dependent_state_best_val = bestActionValue(mdp.mapping[NonTerminal(dependent_state)])\n",
    "            stateNewGap = abs(v[NonTerminal(dependent_state)] - dependent_state_best_val)\n",
    "            if stateNewGap == 0:\n",
    "                continue\n",
    "            if not(gaps.isEmpty()) and gaps.contains(dependent_state):\n",
    "                gaps.update(dependent_state, -stateNewGap)\n",
    "            else:\n",
    "                gaps.insert(dependent_state, -stateNewGap)\n",
    "\n",
    "            \n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        perform a single update to v for the state with the largest gap\n",
    "        update the gaps for any dependent states\n",
    "        '''\n",
    "        #raise NotImplementedError \n",
    "        '''END YOUR CODE'''\n",
    "        return v\n",
    "\n",
    "    \n",
    "    return iterate(update, v_0)\n",
    "\n",
    "\n",
    "def gaps_value_iteration_result(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    gamma: float\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    \n",
    "    gaps = PriorityQueue()\n",
    "\n",
    "    v: V[S] = {s: 0.0 for s in mdp.non_terminal_states}\n",
    "    \n",
    "    def bestActionValue(state_action_map): \n",
    "        action_value_map: Dict[A, float] = {}\n",
    "\n",
    "        for action in state_action_map.keys():\n",
    "            dist_map = state_action_map[action]\n",
    "\n",
    "            reward: float = 0\n",
    "            future_value: float = 0\n",
    "\n",
    "            for stateReward, prob in dist_map:\n",
    "                reward += prob * stateReward[1]\n",
    "                if isinstance(stateReward[0], NonTerminal):\n",
    "                    future_value += gamma * prob * v[stateReward[0]]\n",
    "\n",
    "            action_value_map[action] = future_value + reward\n",
    "\n",
    "        return max(action_value_map.values())\n",
    "    \n",
    "    for s in mdp.non_terminal_states:\n",
    "        sBestVal = -abs(bestActionValue(mdp.mapping[s]))\n",
    "        if not(gaps.isEmpty()) and sBestVal < gaps.peek()[0]:\n",
    "            gaps.pop()\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "        if not(gaps.isEmpty()) and sBestVal == gaps.peek()[0] and not(gaps.contains(s.state)):\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "        elif gaps.isEmpty():\n",
    "            gaps.insert(s.state, sBestVal)\n",
    "            \n",
    "    '''\n",
    "    YOUR CODE HERE\n",
    "    instantiate the value function and populate the gaps\n",
    "    ''' \n",
    "    #raise NotImplementedError\n",
    "    ''' END YOUR CODE ''' \n",
    "    \n",
    "    def criterion(x,y):\n",
    "        THRESHOLD = 1e-5\n",
    "        return (gaps.isEmpty()) or (abs(gaps.peek()[0]) < THRESHOLD)\n",
    "        ''' END YOUR CODE ''' \n",
    "        \n",
    "    opt_vf: V[S] = converged(\n",
    "        gaps_value_iteration(mdp, gamma, gaps),\n",
    "        done= criterion \n",
    "    )\n",
    "        \n",
    "    opt_policy: markov_decision_process.FiniteDeterministicPolicy[S, A] = dynamic_programming.greedy_policy_from_vf(\n",
    "        mdp,\n",
    "        opt_vf,\n",
    "        gamma\n",
    "    )\n",
    "\n",
    "    return opt_vf, opt_policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not change the code below here, just run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with sparse rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "underlying_maze = grid_maze.Maze(50, 50)\n",
    "maze_mdp = grid_maze.GridMazeMDP_Sparse(underlying_maze, 0, 0)\n",
    "#print(maze_mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14131689071655273\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 0.9)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8211369514465332\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 0.9)\n",
    "#print(v2_res)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the VF for a maze with dense rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_mdp = grid_maze.GridMazeMDP_Dense(underlying_maze, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### printing the runtime for the calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4453179836273193\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v1_res = gaps_value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8264849185943604\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "v2_res = dynamic_programming.value_iteration_result(maze_mdp, 1)\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### confirming that the value functions are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert v1_res[1] == v2_res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
