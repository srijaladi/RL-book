#!/usr/bin/env python3# -*- coding: utf-8 -*-from dataclasses import dataclassfrom typing import Optional, Mappingimport numpy as npimport itertoolsfrom rl.distribution import (Categorical, Distribution, FiniteDistribution,                             SampledDistribution, Constant)from rl.markov_process import MarkovProcess, NonTerminal, State, Terminalfrom rl.gen_utils.common_funcs import get_logistic_func, get_unit_sigmoid_funcfrom rl.markov_process import FiniteMarkovProcess, MarkovRewardProcess, FiniteMarkovRewardProcessfrom rl.markov_decision_process import (FiniteMarkovDecisionProcess,                                        FiniteMarkovRewardProcess)from rl.policy import FinitePolicy, FiniteDeterministicPolicyfrom rl.chapter2.stock_price_simulations import\    plot_single_trace_all_processesfrom rl.chapter2.stock_price_simulations import\    plot_distribution_at_time_all_processesimport matplotlib         from matplotlib import pyplot as pltfrom typing import (Callable, Dict, Iterable, Generic, Sequence, Tuple,                    Mapping, TypeVar, Set)import itertoolsfrom itertools import combinations@dataclass(frozen=True)class StatePad:    curr_pad: intclass frogGameDecision(FiniteMarkovDecisionProcess[StatePad, int],                        FiniteMarkovRewardProcess[StatePad]):        def __init__(self, numPads: int):        self.numPads = numPads        self.maxPad = numPads - 1        super().__init__(self.get_action_transition_reward_map())            def get_action_transition_reward_map(self):        d: Dict[StatePad, Dict[int, Categorical[Tuple[StatePad, float]]]] = {}        n = self.maxPad        for i in range(1,self.maxPad):            #Action A (0) taken            action_a_map = {}                        if i+1 == n:                action_a_map[(StatePad(i+1), 1)] = 1 - (i/n)            else:                action_a_map[(StatePad(i+1), 0)] = 1 - (i/n)               action_a_map[(StatePad(i-1), 0)] = i/n                        #Action B (1) taken            action_b_map = {}                        for j in range(0,self.maxPad):                if i == j:                    continue                 action_b_map[(StatePad(j), 0)] = 1/n                action_b_map[(StatePad(self.maxPad), 1)] = 1/n                        action_transition_reward_map_pad = {0: Categorical(action_a_map),                                                1: Categorical(action_b_map)}                        d[StatePad(i)] = action_transition_reward_map_pad                return d   startN = 3endN = 9iterAmount = 3for nVal in range(startN, endN+1, iterAmount):    n: int = nVal    input_pad_size = n+1    gamma: int = 1    allPads = [StatePad(i) for i in range(0,input_pad_size)]    allValidPads = [StatePad(i) for i in range(1,n)]        frog_mdp = frogGameDecision(numPads = input_pad_size)        #print(frog_mdp.get_action_transition_reward_map())        deterministicPolicies: [FiniteDeterministicPolicy[StatePad, int]] = []        for numChange in range(0, n):        allPossChanges = list(combinations(allValidPads, numChange))                for listToChange in allPossChanges:            currPolicy = {StatePad(i):0 for i in range(1,n)}                        for padToChange in listToChange:                currPolicy[padToChange] = 1                        deterministicPolicies.append(FiniteDeterministicPolicy(currPolicy))                            #print(deterministicPolicies)        deterministicPoliciesMRP = [frog_mdp.apply_finite_policy(eachPolicy)                                 for eachPolicy in deterministicPolicies]        valueFunctions: [np.ndarray] = [fmrp.get_value_function_vec(gamma)                                     for fmrp in deterministicPoliciesMRP]    valueFunctionSums: int = [sum(value_vec) for value_vec in valueFunctions]        optimalValueFunctionSum = max(valueFunctionSums)        optimalIndex = valueFunctionSums.index(optimalValueFunctionSum)        optimalValueFunction = valueFunctions[optimalIndex]    optimalDeterministicPolicy = deterministicPolicies[optimalIndex]        print("Optimal Total Value Function Sum: " + str(optimalValueFunctionSum))    print("Optimal Deterministic Policy:\n" + str(optimalDeterministicPolicy))    print("Optimal Value Function Vector: " + str(optimalValueFunction))        graphArrX = np.array([], int)    graphArrY = np.array([], int)        for curr_frog_pad, curr_frog_action in optimalDeterministicPolicy.action_for.items():        graphArrX = np.append(graphArrX, curr_frog_pad.curr_pad)        graphArrY = np.append(graphArrY, curr_frog_action)        plt.clf()    plt.plot(graphArrX, graphArrY)    plt.show()"""Note that in the graphs, at any integer x value, the corresponding y valuerepresents the optimal deterministic policy at that state(x). Each x valueessentially represents a certain frog_pad (1 - (n-1)) and each correspondingy value represents its respective optimal deterministic policy. Also note thata 1 on the y-axis represents an optimal deterministic policy of B and a 0 onthe y-axis represents an optimal deterministic policy of A. The main thing to note is that the optimal deterministicpolicy at every single state BESIDES STATE N-1 is to simply choose Action B (1).Only at State N-1 is the optimal deterministic policy to choose Action A (0). Further, while I did not actually check this, from small experimentation, itseems that for the final state (n - 1), the optimal deterministic is actually any policy as choosing either Action A or B results in the exact same action-valuefunction from that state. """