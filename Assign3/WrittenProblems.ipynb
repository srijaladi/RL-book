{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f386b7b1-7178-4cce-8e72-a8306a5386a8",
   "metadata": {},
   "source": [
    "<h1>Problem 1</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f269917a-af03-4670-9096-fa16b78ae9d7",
   "metadata": {},
   "source": [
    "$V^{\\pi D}(s) = Q^{\\pi D}(s, \\pi_{D}(s))$\n",
    "\n",
    "$V^{\\pi D}(s) = R(s, \\pi_{D}(s)) + \\gamma \\cdot \\sum_{s' \\in S}^{} P(s, \\pi_{D}(s),s') \\cdot Q^{\\pi D}(s', \\pi_{D}(s'))$\n",
    "\n",
    "$V^{\\pi D}(s) = R(s, \\pi_{D}(s)) + \\gamma \\cdot \\sum_{s' \\in S}^{} P(s, \\pi_{D}(s),s') \\cdot V^{\\pi D}(s')$\n",
    "\n",
    "$Q^{\\pi D}(s, \\pi_{D}(s)) = R(s, \\pi_{D}(s)) + \\gamma \\cdot \\sum_{s' \\in S}^{} P(s, \\pi_{D}(s),s') \\cdot V^{\\pi D}(s')$\n",
    "\n",
    "$Q^{\\pi D}(s, \\pi_{D}(s)) = R(s, \\pi_{D}(s)) + \\gamma \\cdot \\sum_{s' \\in S}^{} P(s, \\pi_{D}(s),s') \\cdot Q^{\\pi D}(s', \\pi_{D}(s'))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e19cbc-9e84-4d49-84c6-2461da7c650b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41eb5000-0074-4369-a879-3e075c68cb2b",
   "metadata": {},
   "source": [
    "<h1>Problem 2</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4809c9fa-7157-41d9-90e5-c99e2223336d",
   "metadata": {},
   "source": [
    "\n",
    "The conditions we know are:\n",
    "\n",
    "$P(s+1 | s,a) = a$\n",
    "\n",
    "$P(s | s,a) = 1-a$\n",
    "\n",
    "$R_{T}(s,a,s+1) = 1-a$\n",
    "\n",
    "$R_{T}(s,a,s) = 1+a$\n",
    "\n",
    "\n",
    "Using the formula for $V^{*}(s)$ and using $a$ as our action at each state gives:\n",
    "\n",
    "$V^{*}(s) = max_{a\\in [0,1]}(V^{a}(s))$ for all $s\\in N$\n",
    "\n",
    "Assuming a is any action to be taken, we can create a general value function as:\n",
    "\n",
    "$V^{a}(s) = P(s,a,s+1) \\cdot R_{T}(s,a,s+1) + P(s,a,s) \\cdot R_{T}(s,a,s) + \\gamma \\cdot (P(s,a,s) \\cdot V^{a}(s) + P(s,a,s+1) \\cdot V^{a}(s+1))$\n",
    "\n",
    "At this stage there are two things we are going to do: \n",
    "First, Replace all the P,R functions with their values from above.\n",
    "Second, Notice that $V^{a}(s) = V^{a}(s+n)$ for any n, and replace $V^{a}(s+n)$ with $V^{a}(s)$. This gives a resulting function of:\n",
    "\n",
    "$V^{a}(s) = a \\cdot (1-a) + (1-a) \\cdot (1+a) + \\gamma \\cdot ((1-a) \\cdot V^{a}(s) + (a) \\cdot V^{a}(s))$\n",
    "\n",
    "$V^{a}(s) = a \\cdot (1-a) + (1-a) \\cdot (1+a) + \\frac{1}{2} \\cdot V^{a}(s)$\n",
    "\n",
    "$\\frac{1}{2} \\cdot V^{a}(s) = 1 + a - 2a^{2}$\n",
    "\n",
    "$V^{a}(s) = 2 + 2a - 4a^{2}$\n",
    "\n",
    "\n",
    "Thus, to maximize the value function, we simply need to maximize the function:\n",
    "\n",
    "$2 + 2a - 4a^{2}$\n",
    "\n",
    "After taking the derivative, the maximum value of this function occurs when $a = 0.25$ and the value of the function at $a = 0.25$ is $0.5$. Thus, we now have the optimal value function and optimal policy at every state to be:\n",
    "\n",
    "$V^{\\pi(s) = 0.25}(s) = 0.5$ and $\\pi^{*}(s) = 0.25$ at all s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190e1b5-ed91-4b82-8fce-89063b46e92e",
   "metadata": {},
   "source": [
    "<h1>Problem 3</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616613e7-14d7-46ef-ab83-985975514e41",
   "metadata": {},
   "source": [
    "The state space of this problem can be expressed as $s \\in S = \\{s_0,s_1,s_2,..., s_n\\}$ where $S_{Terminal} = \\{s_0, s_n\\}$ and $S_{Non-Terminal} = \\{s_1,s_2,..., s_{n-1}\\}$.\n",
    "\n",
    "The action space of this problem can be expressed as *A* $\\in \\{A,B\\}$\n",
    "\n",
    "The transition functions for Actions $A, B$ can be expressed as:\n",
    "\n",
    "$P(s_i, A, s_{i+1}) = \\frac{i}{n}$, $P(s_i, A, s_{i-1}) = \\frac{n-i}{n}$, and $P(s_i, A, s_{k}) = 0$ when $k \\notin \\{i-1, i+1\\}$\n",
    "\n",
    "For Action $B$:\n",
    "\n",
    "$P(s_i, B, s_{k}) = \\frac{1}{n}$ for all $i \\in \\{1,2,3...,n-1\\}$ and $k \\in \\{0,1,2...n\\}$\n",
    "\n",
    "Finally, for reward, we are simply going to reward 1 whenever the frog reaches state $s_n$ and 0 for every other transition and state. In other words our Reward functions can be expressed as:\n",
    "\n",
    "$R(s_{n-1}, A, s_n) = 1$ and $R(s_i, B, s_n) = 1$ where $i \\in \\{1,2,3,4,5...n-1\\}$. For all others, $R(s_i, A/B, s_k) =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd45741-db79-4698-a663-148aa6d4d7d6",
   "metadata": {},
   "source": [
    "<h1>Problem 4</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e15ed-ef2c-4b92-850a-228c4fc44bbc",
   "metadata": {},
   "source": [
    "Since we are focused on when $\\gamma = 0$, we know that we are only focused on the next step of the process or the corresponding cost associated with moving to $s'$. We know that the corresponding cost of moving to $s'$ can be expressed as $e^{as'}$ where $a$ is a number that we select. Our goal here is to minimize the expected vlaue of $e^{as'}$ for all $s'$ given a choice of a.\n",
    "\n",
    "First notice that $s' ~ N(s, \\sigma^{2})$. Thus, $as'$ is distributed as $N(as, a^2 \\cdot \\sigma^{2})$. Essentially we are trying to minimize the expected value of $e^X$ where $X ~ N(as, a^2 \\cdot \\sigma^{2})$. This is a log-normal distribution, meaning that $E\\{e^x\\}$ = $e^{(\\mu + 0.5 \\cdot \\sigma^{2})}$\n",
    "\n",
    "We know that the mean of $X$ is $as$ and the standard deviation is $a^2 \\cdot \\sigma^{2}$, so the expected value can be expressed as:\n",
    "\n",
    "$e^{(as + 0.5 \\cdot (a^2 \\cdot \\sigma^{2}))}$. \n",
    "\n",
    "Given that we want to minimize this expected value, we are going to take the derivative of this with respect to a to get:\n",
    "\n",
    "$(s + a \\cdot \\sigma^{2}) \\cdot e^{(as + 0.5 \\cdot (a^2 \\cdot \\sigma^{2}))}$. This equals 0 when $a = -\\frac{s}{\\sigma^2}$. Through plugging this in, we can verify that this does indeed create a minimum, and thus, to minimize the cost, the optimal action at any state is:\n",
    "\n",
    "$a = -\\frac{s}{\\sigma^2}$. Further, the expected corresponding cost is:\n",
    "\n",
    "$e^{(-\\frac{s^2}{\\sigma^2} + 0.5 \\cdot \\frac{s^2}{\\sigma^2})} = e^{(0.5 \\cdot \\frac{s^2}{\\sigma^2})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0661f-2d5d-403a-8ce2-b3ba6c1af72e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93153c6-dcad-4f6f-a266-11de18afb47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
